{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2KmlokN1bSn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "env._max_episode_steps=200"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEfBIGmPn3Ac"
      },
      "source": [
        "class Reinforce(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,env):\n",
        "        super(Reinforce, self).__init__()\n",
        "        self.env=env\n",
        "        self.MAX_TRAJ=200\n",
        "        self.max_reward=200\n",
        "        self.space_size=env.observation_space.shape\n",
        "        self.action_size=env.action_space.n\n",
        "        self.learning_rate=0.01\n",
        "        self.gamma=0.999\n",
        "        self.optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.layer1=tf.keras.layers.Dense(128,\"relu\",input_shape=self.space_size)\n",
        "        self.layer2=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer3=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer4=tf.keras.layers.Dense(self.action_size,\"softmax\")\n",
        "        \n",
        "    def call_model(self,state):\n",
        "        net=self.layer1(state)\n",
        "        net=self.layer2(net)\n",
        "        net=self.layer3(net)\n",
        "        net=self.layer4(net)\n",
        "        return net\n",
        "    \n",
        "    def action(self,state):\n",
        "        s=state.reshape(1,-1)\n",
        "        net=self.call_model(s)\n",
        "        action=tfp.distributions.Categorical(probs=net).sample()\n",
        "        return action.numpy()[0]\n",
        "      \n",
        "    def get_log_prob(self,state,action):\n",
        "        #s=state.reshape(1,-1)\n",
        "        net=self.call_model(state)\n",
        "        dist=tfp.distributions.Categorical(probs=net)\n",
        "        return dist.log_prob(action)\n",
        "      \n",
        "    def get_return_G(self,rewards):\n",
        "        G=[]\n",
        "        for i in range(len(rewards)):\n",
        "            ret=0\n",
        "            p=0\n",
        "            for j in rewards[i:]:\n",
        "                ret+= self.gamma**p*j\n",
        "                p+=1\n",
        "            G.append([ret])\n",
        "        return G\n",
        "\n",
        "    def get_trajectory(self):\n",
        "        states=[]\n",
        "        actions=[]\n",
        "        rewards=[]\n",
        "        \n",
        "        done=False\n",
        "        curr_state=env.reset()\n",
        "        \n",
        "        while not done:\n",
        "            act=self.action(curr_state)\n",
        "            next_state,reward,done,_=env.step(act)\n",
        "            states.append(curr_state)\n",
        "            actions.append([act])\n",
        "            rewards.append(reward)\n",
        "            curr_state=next_state\n",
        "        return states,actions,rewards\n",
        "\n",
        "    def get_full_experience(self):\n",
        "        all_states=[]\n",
        "        all_actions=[]\n",
        "        all_G=[]\n",
        "        current_value=0\n",
        "        for i in range(self.MAX_TRAJ):\n",
        "            states,actions,rewards=self.get_trajectory()\n",
        "            G=self.get_return_G(rewards)\n",
        "            all_states.extend(states)\n",
        "            all_actions.extend(actions)\n",
        "            all_G.extend(G)\n",
        "            current_value+=sum(rewards)\n",
        "        current_value/=self.MAX_TRAJ\n",
        "        return np.array(all_states),np.array(all_actions),np.array(all_G),current_value\n",
        "\n",
        "    def loss(self,states,actions,returns):\n",
        "        m=states.shape[0]\n",
        "        log_prob=self.get_log_prob(states,actions)\n",
        "        return -1* tf.reduce_sum(tf.multiply(log_prob,returns))/m\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        max_value=self.max_reward \n",
        "        curr_value=0\n",
        "\n",
        "        counter=1\n",
        "\n",
        "        while curr_value < max_value:\n",
        "            states,actions,returns,curr_value=self.get_full_experience()\n",
        "            print(f\"At iteration {counter} value_function: {curr_value}\")\n",
        "\n",
        "            with tf.GradientTape() as t:\n",
        "                J=self.loss(states,actions,returns)\n",
        "            grads=t.gradient(J,self.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(grads,self.trainable_variables))\n",
        "            \n",
        "            counter+=1\n",
        "    \n",
        "    def test(m=1):\n",
        "      for i in range(m):\n",
        "          curr_state=self.env.reset()\n",
        "          done=False\n",
        "          rewd=0\n",
        "          while not done:\n",
        "              self.env.render()\n",
        "              act=self.action(curr_state)\n",
        "              next_state,reward,done,_=self.env.step(act)\n",
        "\n",
        "              curr_state=next_state\n",
        "              rewd+=reward\n",
        "          print(f\"{i}: Total Reward:{rewd}\")\n",
        "      env.close()\n",
        "      env.display()\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xUPVkT2p9r4"
      },
      "source": [
        "agent=Reinforce(env)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs8RpK4vy6L8"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrGwM00TIS3l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}