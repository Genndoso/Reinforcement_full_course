{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fV2oTwPw6bbb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rWdqvbjHBy63"
   },
   "outputs": [],
   "source": [
    "class Agent(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,env):\n",
    "      super(Agent, self).__init__()\n",
    "      self.env=env\n",
    "      self.MAX_TRAJ=200\n",
    "      self.space_size=env.observation_space.shape\n",
    "      self.action_size=env.action_space.n\n",
    "      self.learning_rate=0.005\n",
    "      self.gamma=0.999\n",
    "      \n",
    "      self.optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "      \n",
    "      self.layer1=tf.keras.layers.Dense(128,\"relu\",input_shape=self.space_size)\n",
    "      self.layer2=tf.keras.layers.Dense(128,\"relu\")\n",
    "      self.layer3=tf.keras.layers.Dense(128,\"relu\")\n",
    "      self.layer4=tf.keras.layers.Dense(self.action_size,\"softmax\")\n",
    "\n",
    "    def call(self,state):\n",
    "        net=self.layer1(state)\n",
    "        net=self.layer2(net)\n",
    "        net=self.layer3(net)\n",
    "        net=self.layer4(net)\n",
    "        return net\n",
    "    \n",
    "    def action(self,state):\n",
    "        s=state.reshape(1,-1)\n",
    "        net=self.call(s)\n",
    "        action=tfp.distributions.Categorical(probs=net).sample()\n",
    "        return action.numpy()[0]\n",
    "      \n",
    "    def get_log_prob(self,state,action):\n",
    "        net=self.call(state)\n",
    "        dist=tfp.distributions.Categorical(probs=net)\n",
    "        return dist.log_prob(action)\n",
    "      \n",
    "    def get_return_G(self,rewards):\n",
    "        G=[]\n",
    "        for i in range(len(rewards)):\n",
    "            ret=0\n",
    "            p=0\n",
    "            for j in rewards[i:]:\n",
    "                ret+= self.gamma**p*j\n",
    "                p+=1\n",
    "            G.append([ret])\n",
    "        return G\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        \n",
    "        done=False\n",
    "        curr_state=env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            act=self.action(curr_state)\n",
    "            next_state,reward,done,_=env.step(act)\n",
    "            states.append(curr_state)\n",
    "            actions.append([act])\n",
    "            rewards.append(reward)\n",
    "            curr_state=next_state\n",
    "        return states,actions,rewards\n",
    "\n",
    "    def get_full_experience(self):\n",
    "        all_states=[]\n",
    "        all_actions=[]\n",
    "        all_G=[]\n",
    "        current_value=0\n",
    "        for i in range(self.MAX_TRAJ):\n",
    "            states,actions,rewards=self.get_trajectory()\n",
    "            G=self.get_return_G(rewards)\n",
    "            all_states.extend(states)\n",
    "            all_actions.extend(actions)\n",
    "            all_G.extend(G)\n",
    "            current_value+=sum(rewards)\n",
    "        current_value/=self.MAX_TRAJ\n",
    "        return np.array(all_states),np.array(all_actions),np.array(all_G),current_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "bDgImN2QGlus"
   },
   "outputs": [],
   "source": [
    "class Base_line(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,env):\n",
    "      super(Base_line , self).__init__()\n",
    "      self.env=env\n",
    "      self.space_size=env.observation_space.shape\n",
    "      self.action_size=env.action_space.n\n",
    "      self.base_line_optimizer=tf.keras.optimizers.Adam()\n",
    "      \n",
    "      self.layer1=tf.keras.layers.Dense(128,\"relu\",input_shape=self.space_size)\n",
    "      self.layer2=tf.keras.layers.Dense(128,\"relu\")\n",
    "      self.layer3=tf.keras.layers.Dense(128,\"relu\")\n",
    "      self.layer4=tf.keras.layers.Dense(self.action_size,\"softmax\")\n",
    "\n",
    "    def call(self,s):\n",
    "        net=self.layer1(s)\n",
    "        net=self.layer2(net)\n",
    "        net=self.layer3(net)\n",
    "        net=self.layer4(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gW2dstqi6q4K"
   },
   "outputs": [],
   "source": [
    "class Vanilla_Policy_Gradient(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        super(Vanilla_Policy_Gradient, self).__init__()\n",
    "        self.env=env\n",
    "        self.Agent=Agent(env)\n",
    "        self.Base_line=Base_line(env)\n",
    "        self.max_reward=200\n",
    "\n",
    "        self.Base_line.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "    def loss(self,states,actions,Advantage):\n",
    "        m=states.shape[0]\n",
    "        log_prob=self.Agent.get_log_prob(states,actions)\n",
    "        return -1* tf.reduce_sum(tf.multiply(log_prob,Advantage))/m\n",
    "    \n",
    "    def train(self):\n",
    "      max_value=self.max_reward \n",
    "      curr_value=0\n",
    "\n",
    "      counter=1\n",
    "      while curr_value < max_value:\n",
    "            states,actions,returns,curr_value=self.Agent.get_full_experience()\n",
    "\n",
    "            print(f\"At iteration {counter} value_function: {curr_value}\")\n",
    "\n",
    "            pred=self.Base_line.predict(states)\n",
    "            G=[]\n",
    "            for i in range(len(pred)):\n",
    "              G.append([pred[i][actions[i][0]]])\n",
    "\n",
    "            G_n=np.array(G)\n",
    "            Adv=returns-(G_n)\n",
    " \n",
    "            Adv=(Adv-Adv.mean())/Adv.std()\n",
    "\n",
    "            self.Base_line.fit(states,returns,epochs=100,verbose=0)\n",
    "\n",
    "            with tf.GradientTape() as t:\n",
    "                J=self.loss(states,actions,Adv)\n",
    "            \n",
    "            grads=t.gradient(J,self.Agent.trainable_variables)\n",
    "            self.Agent.optimizer.apply_gradients(zip(grads,self.Agent.trainable_variables))\n",
    "            \n",
    "            counter+=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "h78qxoYqGYpf"
   },
   "outputs": [],
   "source": [
    "Agent=Vanilla_Policy_Gradient(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4WHoqDRPFEE"
   },
   "outputs": [],
   "source": [
    "Agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5euUWQEVkCOi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vanilla_Policy_Gradient.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
