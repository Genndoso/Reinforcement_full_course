{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce_stable.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tmn3mhJCSWx"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLA-d5LCVe2"
      },
      "source": [
        "class Agent(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Agent, self).__init__()\n",
        "        self.layer1=tf.keras.layers.Dense(128,\"relu\",input_shape=(4,))\n",
        "        self.layer2=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer3=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer4=tf.keras.layers.Dense(2,\"softmax\")\n",
        "        \n",
        "    def call(self,s):\n",
        "        net=self.layer1(s)\n",
        "        net=self.layer2(net)\n",
        "        net=self.layer3(net)\n",
        "        net=self.layer4(net)\n",
        "        return net\n",
        "    \n",
        "    def action(self,s):\n",
        "        s=s.reshape(1,-1)\n",
        "        net=self.call(s)\n",
        "        act=tfp.distributions.Categorical(probs=net).sample()\n",
        "        return act.numpy()[0]\n",
        "    \n",
        "    def log_prob(self,s,a):\n",
        "        net=self.call(s)\n",
        "        dist=tfp.distributions.Categorical(probs=net)\n",
        "        return dist.log_prob(a)\n",
        "    \n",
        "    def loss(self,S,A,G):\n",
        "      m=S.shape[0]\n",
        "      log_pi_A_given_S=agent.log_prob(S,A)\n",
        "      return -1* tf.reduce_sum(tf.multiply(log_pi_A_given_S,G))/m\n",
        "    \n",
        "    def get_returns_from_rewards(self,rewards):\n",
        "      G=[]\n",
        "      ret=0\n",
        "      for r in reversed(rewards):\n",
        "          ret=0.99*ret+r #gamma=0.99\n",
        "          G.insert(0,ret)\n",
        "      return np.array(G)\n",
        "    \n",
        "    def get_trajectory(self):\n",
        "      states=[]\n",
        "      actions=[]\n",
        "      rewards=[]\n",
        "\n",
        "      curr_state=env.reset()\n",
        "      done=False\n",
        "\n",
        "      while not done:\n",
        "          act=agent.action(curr_state)\n",
        "          next_state,reward,done,_=env.step(act)\n",
        "\n",
        "          states.append(curr_state)\n",
        "          actions.append([act])\n",
        "          rewards.append(reward)\n",
        "\n",
        "          curr_state=next_state\n",
        "\n",
        "      G=self.get_returns_from_rewards(rewards).reshape(-1,1).tolist()\n",
        "      R=sum(rewards)\n",
        "      return states,actions,G,R\n",
        "    \n",
        "    def get_experience_on_theta(self,m):\n",
        "      states=[];actions=[];G=[];V_theta=0\n",
        "      for i in range(m):\n",
        "          S,A,g,R=self.get_trajectory()\n",
        "          states.extend(S)\n",
        "          actions.extend(A)\n",
        "          G.extend(g)\n",
        "          V_theta+=R\n",
        "      return np.array(states),np.array(actions),np.array(G),V_theta/m\n",
        "\n",
        "    def train(self):\n",
        "      goal_vtheta=200\n",
        "      v_theta=0\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "      i=1\n",
        "      while v_theta <goal_vtheta:\n",
        "          S,A,G,v_theta=self.get_experience_on_theta(200) #m=200\n",
        "          print(f\"{i}: V_theta:{v_theta}\")\n",
        "          \n",
        "          \n",
        "          with tf.GradientTape() as t:\n",
        "              J=self.loss(S,A,G)\n",
        "          grads=t.gradient(J,self.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(grads,self.trainable_variables))\n",
        "          \n",
        "          i+=1\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqWgHR8Cl_p"
      },
      "source": [
        "agent=Agent()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6JGsgLbgVzE"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fYnRS9cFOcA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}