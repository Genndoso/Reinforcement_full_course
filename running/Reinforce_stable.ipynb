{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce_stable.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tmn3mhJCSWx"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLA-d5LCVe2"
      },
      "source": [
        "class Agent(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Agent, self).__init__()\n",
        "        self.layer1=tf.keras.layers.Dense(128,\"relu\",input_shape=(4,))\n",
        "        self.layer2=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer3=tf.keras.layers.Dense(128,\"relu\")\n",
        "        self.layer4=tf.keras.layers.Dense(1,\"sigmoid\")\n",
        "        \n",
        "    def call(self,s):\n",
        "        net=self.layer1(s)\n",
        "        net=self.layer2(net)\n",
        "        net=self.layer3(net)\n",
        "        net=self.layer4(net)\n",
        "        return net\n",
        "    \n",
        "    def action(self,s):\n",
        "        s=s.reshape(1,-1)\n",
        "        net=self.call(s)\n",
        "        act=tfp.distributions.Bernoulli(probs=net).sample()\n",
        "        return act.numpy()[0]\n",
        "    \n",
        "    def log_prob(self,s,a):\n",
        "        net=self.call(s)\n",
        "        dist=tfp.distributions.Bernoulli(probs=net)\n",
        "        return dist.log_prob(a)\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqWgHR8Cl_p"
      },
      "source": [
        "agent=Agent()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVakIo9GCqBR"
      },
      "source": [
        "def loss(S,A,G):\n",
        "    m=S.shape[0]\n",
        "    log_pi_A_given_S=agent.log_prob(S,A)\n",
        "    return -1* tf.reduce_sum(tf.multiply(log_pi_A_given_S,G))/m"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd8abM28C5pM"
      },
      "source": [
        "import gym\n",
        "env=gym.make(\"CartPole-v0\")\n",
        "env._max_episode_steps=200"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0AFLvCyC9DJ"
      },
      "source": [
        "def get_returns_from_rewards(rewards):\n",
        "    G=[]\n",
        "    ret=0\n",
        "    for r in reversed(rewards):\n",
        "        ret=0.99*ret+r #gamma=0.99\n",
        "        G.insert(0,ret)\n",
        "    return np.array(G)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQELI86xC_hN"
      },
      "source": [
        "def get_trijectory():\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    rewards=[]\n",
        "\n",
        "    curr_state=env.reset()\n",
        "    done=False\n",
        "\n",
        "    while not done:\n",
        "        act=agent.action(curr_state)\n",
        "        next_state,reward,done,_=env.step(act[0])\n",
        "\n",
        "        states.append(curr_state)\n",
        "        actions.append(act)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        curr_state=next_state\n",
        "\n",
        "    G=get_returns_from_rewards(rewards).reshape(-1,1).tolist()\n",
        "    R=sum(rewards)\n",
        "    return states,actions,G,R"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsnCK65TDCIb"
      },
      "source": [
        "def get_experience_on_theta(m):\n",
        "    states=[];actions=[];G=[];V_theta=0\n",
        "    for i in range(m):\n",
        "        S,A,g,R=get_trijectory()\n",
        "        states.extend(S)\n",
        "        actions.extend(A)\n",
        "        G.extend(g)\n",
        "        V_theta+=R\n",
        "    return np.array(states),np.array(actions),np.array(G),V_theta/m"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hty3RPqHDk5S"
      },
      "source": [
        "def train():\n",
        "  goal_vtheta=200\n",
        "  v_theta=0\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  i=1\n",
        "  while v_theta <goal_vtheta:\n",
        "      S,A,G,v_theta=get_experience_on_theta(200) #m=200\n",
        "      print(f\"{i}: V_theta:{v_theta}\")\n",
        "      \n",
        "      \n",
        "      with tf.GradientTape() as t:\n",
        "          J=loss(S,A,G)\n",
        "      grads=t.gradient(J,agent.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads,agent.trainable_variables))\n",
        "      \n",
        "      i+=1\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8wRYNyjDqtd"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fYnRS9cFOcA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}