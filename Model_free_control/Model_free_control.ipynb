{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Free Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"question.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gridworld import GridWorld\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "world=\\\n",
    "    \"\"\"\n",
    "    wwwwwwwwww\n",
    "    wa       w\n",
    "    w    wwwww\n",
    "    wwww     w\n",
    "    w        w\n",
    "    w        w\n",
    "    w        w\n",
    "    w g  wwwww\n",
    "    w        w\n",
    "    wwwwwwwwww\n",
    "    \"\"\"\n",
    "env=GridWorld(world)\n",
    "env._max_epi_step=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem_window.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(policy):\n",
    "    episodes=[]\n",
    "    curr_state=env.reset()\n",
    "    done=False\n",
    "    while not done:\n",
    "        action=policy[curr_state]\n",
    "        next_state,reward,done,info=env.step(action)\n",
    "        episodes.append((curr_state,action,reward))\n",
    "        curr_state=next_state\n",
    "    return episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns(episodes,gamma):\n",
    "    returns=[]\n",
    "    \n",
    "    for i,(s,a,r) in enumerate(episodes):\n",
    "        sum_g=r\n",
    "        future=episodes[i+1:]\n",
    "        for j,(sn,an,rn) in enumerate(future):\n",
    "            sum_g+=(gamma**(j+1))*rn\n",
    "        returns.append((s,a,sum_g))\n",
    "    return returns\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(Q,e,actions):\n",
    "    policy=[]\n",
    "    optimal_policy=np.argmax(Q,axis=1)\n",
    "    for s in range(Q.shape[0]):\n",
    "        chosen_action=optimal_policy[s] if np.random.random()<=(1-e) else np.random.choice(actions)\n",
    "        policy.append(chosen_action)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e(epi,steady_e=0.01,steady_epi=1000):\n",
    "    if epi>steady_epi:\n",
    "        return steady_e\n",
    "    else:\n",
    "        m=(steady_e-1)/(steady_epi)\n",
    "        return m*epi+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi():\n",
    "    Q=np.zeros((env.state_count,4))\n",
    "    N=np.zeros((env.state_count,4))\n",
    "    pi=greedy(Q,1,[0,1,2,3])\n",
    "\n",
    "    for epi in tqdm(range(50000)):\n",
    "        epis=returns(episode(pi),0.99)\n",
    "        for s,a,G in epis:\n",
    "            N[s,a]+=1\n",
    "            Q[s,a]+=(1/N[s,a])*(G-Q[s,a])\n",
    "        pi=greedy(Q,get_e(epi,steady_e=0.01, steady_epi=10000),[0,1,2,3])\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [02:48<00:00, 296.35it/s] \n"
     ]
    }
   ],
   "source": [
    "pi=get_pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setPolicy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play_as_human(show_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"route_mc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple():\n",
    "    curr_state=env.reset()\n",
    "    action=policy[curr_state]\n",
    "    while True:\n",
    "        next_state,reward,done,info=env.step(action)\n",
    "        action_next=policy[next_state]\n",
    "        yield(curr_state,action,reward,next_state,action_next)\n",
    "        if not done:\n",
    "            curr_state=next_state\n",
    "            action=action_next\n",
    "        else:\n",
    "            curr_state=env.reset()\n",
    "            action=policy[curr_state]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e(step,steady_e=0.01,steady_step=10000):\n",
    "    if step>steady_step:\n",
    "        return steady_e\n",
    "    else:\n",
    "        m=(steady_e-1)/(steady_step)\n",
    "        return m*step+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(Q,s,e,actions):\n",
    "    optimal_action=np.argmax(Q[s])\n",
    "    chosen_action=optimal_action if np.random.random()<=(1-e) else np.random.choice(actions)\n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi(actions):\n",
    "    Q=np.zeros((env.state_count,4))\n",
    "    global policy\n",
    "    policy=np.random.choice([0,1,2,3],size=env.state_count)\n",
    "    alpha=0.001\n",
    "    gamma=0.99\n",
    "    gen=get_tuple()\n",
    "\n",
    "    for step in tqdm(range(1000000)):\n",
    "        s,a,r,s_p,a_p=next(gen)\n",
    "        Q[s,a]+=alpha*((r+gamma*Q[s_p,a_p])-Q[s,a])\n",
    "        policy[s]=greedy(Q,s,get_e(step,steady_e=0.01,steady_step=100000),[0,1,2,3])\n",
    "    \n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:56<00:00, 17693.28it/s]\n"
     ]
    }
   ],
   "source": [
    "pi=get_pi([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setPolicy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play_as_human(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Route_sarsa.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple():\n",
    "    curr_state=env.reset()\n",
    "    while True:\n",
    "        act=policy[curr_state]\n",
    "        next_state,reward,done,info=env.step(act)\n",
    "        \n",
    "        yield(curr_state,act,reward,next_state)\n",
    "        \n",
    "        if not done:\n",
    "            curr_state=next_state\n",
    "        else:\n",
    "            curr_state=env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi(actions):\n",
    "    Q=np.zeros((env.state_count,4))\n",
    "    global policy\n",
    "    policy=np.random.choice([0,1,2,3],size=env.state_count)\n",
    "    alpha=0.001\n",
    "    gamma=0.99\n",
    "    gen=get_tuple()\n",
    "\n",
    "    for step in tqdm(range(1000000)):\n",
    "        s,a,r,s_p=next(gen)\n",
    "        Q[s,a]+=alpha*((r+gamma*np.max(Q[s_p]))-Q[s,a])\n",
    "        policy[s]=greedy(Q,s,get_e(step,steady_e=0.01,steady_step=100000),[0,1,2,3])\n",
    "    \n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:09<00:00, 14320.58it/s]\n"
     ]
    }
   ],
   "source": [
    "pi=get_pi([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setPolicy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play_as_human(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Route_q.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple():\n",
    "    curr_state=env.reset()\n",
    "    while True:\n",
    "        act=policy[curr_state]\n",
    "        next_state,reward,done,info=env.step(act)\n",
    "        \n",
    "        yield(curr_state,act,reward,next_state)\n",
    "        \n",
    "        if not done:\n",
    "            curr_state=next_state\n",
    "        else:\n",
    "            curr_state=env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi(actions):\n",
    "    Q_max=np.zeros((env.state_count,4))\n",
    "    Q_min=np.zeros((env.state_count,4))\n",
    "    Q = np.zeros((env.state_count,4))\n",
    "    global policy\n",
    "    policy=np.random.choice([0,1,2,3],size=env.state_count)\n",
    "    alpha=0.001\n",
    "    gamma=0.99\n",
    "    gen=get_tuple()\n",
    "\n",
    "    for step in tqdm(range(1000000)):\n",
    "        s,a,r,s_p=next(gen)\n",
    "        Q_max[s,a]+=alpha*((r+gamma*np.max(Q[s_p]))-Q[s,a])\n",
    "        Q_min[s,a]+=alpha*((r+gamma*np.min(Q[s_p]))-Q[s,a])\n",
    "        Q[s,a]=(Q_max[s,a]+Q_min[s,a])/2\n",
    "        policy[s]=greedy(Q,s,get_e(step,steady_e=0.01,steady_step=100000),[0,1,2,3])\n",
    "    \n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:18<00:00, 12759.02it/s]\n"
     ]
    }
   ],
   "source": [
    "pi=get_pi([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setPolicy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play_as_human(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Route_double_q.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
